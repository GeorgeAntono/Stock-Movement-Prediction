{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T16:49:57.266702100Z",
     "start_time": "2024-09-29T16:49:56.020880500Z"
    }
   },
   "id": "f95bd647af02cfc6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\georg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\georg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T16:49:57.401652400Z",
     "start_time": "2024-09-29T16:49:57.269684Z"
    }
   },
   "id": "6cc0d607e8224b98"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "# Load the datasets\n",
    "news_df = pd.read_csv('./data/us_equities_news_dataset.csv')\n",
    "stock_df = pd.read_csv('./data/NVDA.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T16:50:07.192154200Z",
     "start_time": "2024-09-29T16:49:57.402652400Z"
    }
   },
   "id": "dadb60e4b99764da"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total NVIDIA-related articles found: 81038\n",
      "\n",
      "Filtered and Labeled Data (NVIDIA-Related):\n",
      "                                             content     Open    Close  \\\n",
      "0  What s happening\\nShares of Chinese electric c...  6.19475  6.13925   \n",
      "1  Gainers  NIO  NYSE NIO   14   Village Farms In...  6.19475  6.13925   \n",
      "2  Cemtrex  NASDAQ CETX   85  after FY results \\n...  6.19475  6.13925   \n",
      "3  aTyr Pharma  NASDAQ LIFE   63  on Kyorin Pharm...  5.80800  5.92650   \n",
      "4  Gainers  NIO  NYSE NIO   14   Meritor  NYSE MT...  5.77250  5.88250   \n",
      "\n",
      "        Date  target  \n",
      "0 2020-01-15       0  \n",
      "1 2020-01-15       0  \n",
      "2 2020-01-15       0  \n",
      "3 2020-01-06       1  \n",
      "4 2019-12-31       1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_20256\\2552459991.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nvidia_related_articles['Date'] = pd.to_datetime(nvidia_related_articles['release_date'])\n",
      "C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_20256\\2552459991.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nvidia_df['target'] = np.where(nvidia_df['Open'] > nvidia_df['Close'], 0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Training on the Entire Corpus: This will extract a broader set of word embeddings that capture general language patterns and semantics. This way it may result in better generalization.\n",
    "\n",
    "# Sample keywords related to NVIDIA and associated companies\n",
    "\n",
    "# https://www.nvidia.com/en-us/self-driving-cars/partners/nio/ (Nio partnership with NVIDIA)\n",
    "\n",
    "# https://nvidianews.nvidia.com/news/uber-selects-nvidia-technology-to-power-its-self-driving-fleets (Uber partnership with NVIDIA)\n",
    "\n",
    "# https://nvidianews.nvidia.com/news/nvidia-teams-with-amazon-web-services-to-bring-ai-to-millions-of-connected-devices (Amazon partnership with NVIDIA,March 2019 article)\n",
    "\n",
    "# https://www.anandtech.com/show/15146/new-nvidia-gpu-variant-at-supercomputing-2019 (NVIDIA GPU made for TESLA in 2019)\n",
    "\n",
    "nvidia_keywords = [\n",
    "    'NVDA', 'NVIDIA', 'NIO', 'UBER', 'AMZN', 'AMAZON', 'TESLA', 'GPU', 'GRAPHICS',\n",
    "    'CHIP', 'SEMICONDUCTOR', 'DRIVING', 'DEEP LEARNING']\n",
    "\n",
    "# Compile a regex pattern from the keywords list\n",
    "nvidia_pattern = '|'.join(nvidia_keywords)  # Combines the keywords into a regex pattern\n",
    "\n",
    "# Filter articles where the content or ticker column contains any of the keywords\n",
    "nvidia_related_articles = news_df[\n",
    "    news_df['content'].str.contains(nvidia_pattern, case=False, na=False) |\n",
    "    news_df['ticker'].str.contains(nvidia_pattern, case=False, na=False)\n",
    "]\n",
    "\n",
    "# Display the count of NVIDIA-related articles\n",
    "print(f\"\\nTotal NVIDIA-related articles found: {nvidia_related_articles.shape[0]}\")\n",
    "\n",
    "# Convert the date columns to datetime format for matching\n",
    "nvidia_related_articles['Date'] = pd.to_datetime(nvidia_related_articles['release_date'])\n",
    "stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "\n",
    "# Merge filtered news data with stock prices based on publication date\n",
    "merged_df = pd.merge(nvidia_related_articles, stock_df, on='Date', how='inner')\n",
    "\n",
    "# Filter to keep only articles that have matching stock data\n",
    "nvidia_df = merged_df[['content', 'Open', 'Close', 'Date']]\n",
    "\n",
    "# Label the target variable based on the opening and closing prices\n",
    "nvidia_df['target'] = np.where(nvidia_df['Open'] > nvidia_df['Close'], 0, 1)\n",
    "\n",
    "# Display the first few rows to verify the merging and labeling\n",
    "print(\"\\nFiltered and Labeled Data (NVIDIA-Related):\")\n",
    "print(nvidia_df.head())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T16:51:52.283619100Z",
     "start_time": "2024-09-29T16:50:07.192154200Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate documents found: 173\n",
      "Duplicate Documents:\n",
      "                                                 content\n",
      "309    Shares of Uber Technologies Inc  \\r\\n\\r\\n     ...\n",
      "312    Shares of Uber Technologies Inc  \\r\\n\\r\\n     ...\n",
      "625     Bloomberg     You can soon save for your next...\n",
      "640     Bloomberg     You can soon save for your next...\n",
      "5369   Micron  MU  closed the most recent trading day...\n",
      "...                                                  ...\n",
      "68596   Bloomberg     Being an emerging market econom...\n",
      "69812   Bloomberg     A decorated U S Army officer wh...\n",
      "69813   Bloomberg     A decorated U S Army officer wh...\n",
      "70685   Bloomberg     The next U S  jobs report is mo...\n",
      "70686   Bloomberg     The next U S  jobs report is mo...\n",
      "\n",
      "[173 rows x 1 columns]\n",
      "Number of documents after removing duplicates: 71420\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate documents based on the 'content' column\n",
    "duplicate_docs = nvidia_df[nvidia_df['content'].duplicated(keep=False)]\n",
    "\n",
    "# Display the duplicate documents (if any)\n",
    "print(f\"Number of duplicate documents found: {duplicate_docs.shape[0]}\")\n",
    "if not duplicate_docs.empty:\n",
    "    print(\"Duplicate Documents:\")\n",
    "    print(duplicate_docs[['content']])\n",
    "\n",
    "# Remove duplicate documents, keeping the first occurrence\n",
    "nvidia_df = nvidia_df.drop_duplicates(subset='content', keep='first').reset_index(drop=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(f\"Number of documents after removing duplicates: {nvidia_df.shape[0]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T16:51:52.522981100Z",
     "start_time": "2024-09-29T16:51:52.277990500Z"
    }
   },
   "id": "bf72f648a6d1bf6e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of articles before filtering is: 221513\n",
      "The number of articles after filtering is: 71420\n",
      "The average amount of words per article is: 725.113329599552\n"
     ]
    }
   ],
   "source": [
    "# Basic descriptive statistics for the news dataset\n",
    "num_articles = nvidia_df.shape[0]\n",
    "average_words_per_article = nvidia_df['content'].apply(lambda x: len(str(x).split())).mean()\n",
    "print(f'The number of articles before filtering is: {news_df.shape[0]}')\n",
    "print(f\"The number of articles after filtering is: {num_articles}\")\n",
    "print(f\"The average amount of words per article is: {average_words_per_article}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# Tokenize content for word frequency analysis\n",
    "nvidia_df.loc[:, 'text_length'] = nvidia_df['content'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "\n",
    "# Tokenize content for word frequency analysis\n",
    "nvidia_df.loc[:, 'processed_text'] = nvidia_df['content'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "\n",
    "# Remove stopwords and punctuation for better NLP insights\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Remove stopwords, punctuation, and apply stemming\n",
    "nvidia_df.loc[:, 'filtered_text'] = nvidia_df['processed_text'].apply(\n",
    "    lambda words: [stemmer.stem(word) for word in words if word.isalpha() and word not in stop_words]\n",
    ")\n",
    "\n",
    "\n",
    "# Join the filtered words back into strings for TF-IDF\n",
    "nvidia_df.loc[:, 'filtered_text_str'] = nvidia_df['filtered_text'].apply(lambda x: ' '.join(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:04:37.908511400Z",
     "start_time": "2024-09-29T16:51:52.532109Z"
    }
   },
   "id": "d9de1d67fd4bc2f1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# TF-IDF Representation of Documents using the processed and filtered text\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500)  # Reduced max features\n",
    "tfidf_matrix = vectorizer.fit_transform(nvidia_df['filtered_text_str'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:08.409436100Z",
     "start_time": "2024-09-29T17:04:37.911512900Z"
    }
   },
   "id": "544f55b5eff127cd"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Prepare the text data for Word2Vec\n",
    "sentences = nvidia_df['filtered_text'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:05:08.430775200Z",
     "start_time": "2024-09-29T17:05:08.411532800Z"
    }
   },
   "id": "5cef590aa94dfb51"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def document_embedding_tfidf(model, document, tfidf, feature_names, vector_size=100):\n",
    "    \"\"\"\n",
    "    Compute a TF-IDF weighted document embedding by averaging the Word2Vec embeddings of words in the document.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained Word2Vec model\n",
    "    - document: List of tokenized words\n",
    "    - tfidf: TfidfVectorizer object used for the document\n",
    "    - feature_names: The list of features (terms) from the TF-IDF vectorizer\n",
    "    - vector_size: Size of the word embeddings in the Word2Vec model\n",
    "\n",
    "    Returns:\n",
    "    - doc_embedding: The TF-IDF weighted document embedding as a numpy array\n",
    "    \"\"\"\n",
    "    word_vectors = []\n",
    "    weights = []\n",
    "\n",
    "    # Extract the TF-IDF scores for the document\n",
    "    tfidf_scores = tfidf.transform([\" \".join(document)])\n",
    "    tfidf_scores = tfidf_scores.toarray().flatten()\n",
    "\n",
    "    feature_names = feature_names.tolist()  # Convert NumPy array to list\n",
    "\n",
    "    for word in document:\n",
    "        if word in model.wv.index_to_key and word in feature_names:\n",
    "            # Get the word embedding\n",
    "            word_vector = model.wv[word]\n",
    "            word_index = feature_names.index(word)  # Get the index of the word in the TF-IDF feature list\n",
    "\n",
    "            # Get the TF-IDF weight for this word\n",
    "            tfidf_weight = tfidf_scores[word_index]\n",
    "\n",
    "            # Collect the word vector and weight\n",
    "            word_vectors.append(word_vector)\n",
    "            weights.append(tfidf_weight)\n",
    "\n",
    "    if len(word_vectors) == 0:\n",
    "        # Return a zero vector if no words from the document are in the Word2Vec model\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    word_vectors = np.array(word_vectors)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    # Compute the weighted average of word vectors\n",
    "    doc_embedding = np.average(word_vectors, axis=0, weights=weights)\n",
    "\n",
    "    return doc_embedding\n",
    "\n",
    "\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "# Apply the function to all documents in filtered dataframe\n",
    "nvidia_df['doc_embedding'] = nvidia_df['filtered_text'].apply(\n",
    "    lambda doc: document_embedding_tfidf(word2vec_model, doc, vectorizer, tfidf_feature_names)\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:07:11.383435Z",
     "start_time": "2024-09-29T17:07:11.356357600Z"
    }
   },
   "id": "eae61a4e64f76943"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nvector_sizes = [100, 200, 300]\\nwindow_sizes = [3, 5, 7]\\nmin_counts = [1, 5, 10]\\nsg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram\\nepochs = [10, 20]\\n'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the grid of parameters to test\n",
    "'''\n",
    "vector_sizes = [100, 200, 300]\n",
    "window_sizes = [3, 5, 7]\n",
    "min_counts = [1, 5, 10]\n",
    "sg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram\n",
    "epochs = [10, 20]\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T17:07:13.409180100Z",
     "start_time": "2024-09-29T17:07:13.393707900Z"
    }
   },
   "id": "9fbe1bc413181f09"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Size: 200, Window: 3, Min Count: 10, SG: 0, Epochs: 10, Accuracy: 0.5316\n",
      "Vector Size: 200, Window: 3, Min Count: 10, SG: 0, Epochs: 20, Accuracy: 0.5361\n",
      "Vector Size: 200, Window: 3, Min Count: 10, SG: 1, Epochs: 10, Accuracy: 0.5296\n",
      "Vector Size: 200, Window: 3, Min Count: 10, SG: 1, Epochs: 20, Accuracy: 0.5273\n",
      "Best Model Parameters: Vector Size: 200, Window: 3, Min Count: 10, SG: 0, Epochs: 20\n",
      "Best Model Accuracy: 0.5361\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "vector_sizes = [200]\n",
    "window_sizes = [3]\n",
    "min_counts = [10]\n",
    "sg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram\n",
    "epochs = [10, 20]\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "best_score = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for vector_size, window, min_count, sg, epoch in itertools.product(vector_sizes, window_sizes, min_counts, sg_values, epochs):\n",
    "    # Initialize Word2Vec with current hyperparameters\n",
    "    word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=sg)\n",
    "    \n",
    "    # Train the model\n",
    "    word2vec_model.train(sentences, total_examples=len(sentences), epochs=epoch)\n",
    "    \n",
    "    \n",
    "    # Convert sentences to document embeddings\n",
    "    doc_embeddings = [np.mean([word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv], axis=0) for sentence in sentences]\n",
    "    \n",
    "    #Checking empty embeddings\n",
    "    X = np.array([embedding for embedding in doc_embeddings if embedding is not None and len(embedding) > 0])\n",
    "    y = nvidia_df['target']\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, stratify=y)\n",
    "    \n",
    "    # Train a simple classifier (e.g., Logistic Regression) on the embeddings\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the accuracy on the test set\n",
    "    accuracy = lr_model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Vector Size: {vector_size}, Window: {window}, Min Count: {min_count}, SG: {sg}, Epochs: {epoch}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Track the best model\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_params = (vector_size, window, min_count, sg, epoch)\n",
    "        best_model = word2vec_model\n",
    "\n",
    "print(f\"Best Model Parameters: Vector Size: {best_params[0]}, Window: {best_params[1]}, Min Count: {best_params[2]}, SG: {best_params[3]}, Epochs: {best_params[4]}\")\n",
    "print(f\"Best Model Accuracy: {best_score:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T18:04:52.434080800Z",
     "start_time": "2024-09-29T17:07:14.143870700Z"
    }
   },
   "id": "98ec74d1e18b126f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Size: 200, Window: 3, Min Count: 10, SG: 0, Epochs: 10, Accuracy: 0.5144, F1 Score: 0.4946\n",
      "Vector Size: 200, Window: 3, Min Count: 10, SG: 0, Epochs: 20, Accuracy: 0.5147, F1 Score: 0.4972\n",
      "Vector Size: 200, Window: 3, Min Count: 10, SG: 1, Epochs: 10, Accuracy: 0.5123, F1 Score: 0.5111\n",
      "Vector Size: 200, Window: 3, Min Count: 10, SG: 1, Epochs: 20, Accuracy: 0.5138, F1 Score: 0.5123\n",
      "Best Model Parameters: Vector Size: 200, Window: 3, Min Count: 10, SG: 0, Epochs: 20\n",
      "Best Model Accuracy: 0.5147\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "sentences = nvidia_df['filtered_text'].tolist()\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "vector_sizes = [200]\n",
    "window_sizes = [3]\n",
    "min_counts = [10]\n",
    "sg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram\n",
    "epochs = [10, 20]\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "best_score = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for vector_size, window, min_count, sg, epoch in itertools.product(vector_sizes, window_sizes, min_counts, sg_values, epochs):\n",
    "    # Initialize Word2Vec with current hyperparameters\n",
    "    word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=sg)\n",
    "    \n",
    "    # Train the model\n",
    "    word2vec_model.train(sentences, total_examples=len(sentences), epochs=epoch)\n",
    "    \n",
    "    # Convert sentences to document embeddings by averaging the word vectors\n",
    "    doc_embeddings = [np.mean([word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv], axis=0) for sentence in sentences]\n",
    "    \n",
    "    # Checking empty embeddings\n",
    "    X = np.array([embedding for embedding in doc_embeddings if embedding is not None and len(embedding) > 0])\n",
    "    y = nvidia_df['target']\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, stratify=y)\n",
    "    \n",
    "    # Initialize and train Naive Bayes on the document embeddings\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = nb_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model using accuracy and F1-score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Vector Size: {vector_size}, Window: {window}, Min Count: {min_count}, SG: {sg}, Epochs: {epoch}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Track the best model based on accuracy or F1-score\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_params = (vector_size, window, min_count, sg, epoch)\n",
    "        best_model = word2vec_model\n",
    "\n",
    "print(f\"Best Model Parameters: Vector Size: {best_params[0]}, Window: {best_params[1]}, Min Count: {best_params[2]}, SG: {best_params[3]}, Epochs: {best_params[4]}\")\n",
    "print(f\"Best Model Accuracy: {best_score:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-29T18:57:13.037811300Z",
     "start_time": "2024-09-29T18:04:52.476236600Z"
    }
   },
   "id": "4548405f93433bac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9f675327bdfc4648"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
