{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95bd647af02cfc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T11:57:24.809803400Z",
     "start_time": "2024-10-03T11:57:18.367720200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc0d607e8224b98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T11:57:25.072300Z",
     "start_time": "2024-10-03T11:57:24.809803400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\georg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\georg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Ensure NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadb60e4b99764da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T11:57:34.036227500Z",
     "start_time": "2024-10-03T11:57:25.061710400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the datasets\n",
    "news_df = pd.read_csv('./data/us_equities_news_dataset.csv')\n",
    "stock_df = pd.read_csv('./data/NVDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T11:58:57.899894600Z",
     "start_time": "2024-10-03T11:57:34.040736400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total NVIDIA-related articles found: 81038\n",
      "\n",
      "Filtered and Labeled Data (NVIDIA-Related):\n",
      "                                             content     Open    Close  \\\n",
      "0  What s happening\\nShares of Chinese electric c...  6.19475  6.13925   \n",
      "1  Gainers  NIO  NYSE NIO   14   Village Farms In...  6.19475  6.13925   \n",
      "2  Cemtrex  NASDAQ CETX   85  after FY results \\n...  6.19475  6.13925   \n",
      "3  aTyr Pharma  NASDAQ LIFE   63  on Kyorin Pharm...  5.80800  5.92650   \n",
      "4  Gainers  NIO  NYSE NIO   14   Meritor  NYSE MT...  5.77250  5.88250   \n",
      "\n",
      "        Date  target  \n",
      "0 2020-01-15       0  \n",
      "1 2020-01-15       0  \n",
      "2 2020-01-15       0  \n",
      "3 2020-01-06       1  \n",
      "4 2019-12-31       1  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_3068\\2552459991.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nvidia_related_articles['Date'] = pd.to_datetime(nvidia_related_articles['release_date'])\n",
      "C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_3068\\2552459991.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nvidia_df['target'] = np.where(nvidia_df['Open'] > nvidia_df['Close'], 0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Training on the Entire Corpus: This will extract a broader set of word embeddings that capture general language patterns and semantics. This way it may result in better generalization.\n",
    "\n",
    "# Sample keywords related to NVIDIA and associated companies\n",
    "\n",
    "# https://www.nvidia.com/en-us/self-driving-cars/partners/nio/ (Nio partnership with NVIDIA)\n",
    "\n",
    "# https://nvidianews.nvidia.com/news/uber-selects-nvidia-technology-to-power-its-self-driving-fleets (Uber partnership with NVIDIA)\n",
    "\n",
    "# https://nvidianews.nvidia.com/news/nvidia-teams-with-amazon-web-services-to-bring-ai-to-millions-of-connected-devices (Amazon partnership with NVIDIA,March 2019 article)\n",
    "\n",
    "# https://www.anandtech.com/show/15146/new-nvidia-gpu-variant-at-supercomputing-2019 (NVIDIA GPU made for TESLA in 2019)\n",
    "\n",
    "nvidia_keywords = [\n",
    "    'NVDA', 'NVIDIA', 'NIO', 'UBER', 'AMZN', 'AMAZON', 'TESLA', 'GPU', 'GRAPHICS',\n",
    "    'CHIP', 'SEMICONDUCTOR', 'DRIVING', 'DEEP LEARNING']\n",
    "\n",
    "# Compile a regex pattern from the keywords list\n",
    "nvidia_pattern = '|'.join(nvidia_keywords)  # Combines the keywords into a regex pattern\n",
    "\n",
    "# Filter articles where the content or ticker column contains any of the keywords\n",
    "nvidia_related_articles = news_df[\n",
    "    news_df['content'].str.contains(nvidia_pattern, case=False, na=False) |\n",
    "    news_df['ticker'].str.contains(nvidia_pattern, case=False, na=False)\n",
    "]\n",
    "\n",
    "# Display the count of NVIDIA-related articles\n",
    "print(f\"\\nTotal NVIDIA-related articles found: {nvidia_related_articles.shape[0]}\")\n",
    "\n",
    "# Convert the date columns to datetime format for matching\n",
    "nvidia_related_articles['Date'] = pd.to_datetime(nvidia_related_articles['release_date'])\n",
    "stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n",
    "\n",
    "# Merge filtered news data with stock prices based on publication date\n",
    "merged_df = pd.merge(nvidia_related_articles, stock_df, on='Date', how='inner')\n",
    "\n",
    "# Filter to keep only articles that have matching stock data\n",
    "nvidia_df = merged_df[['content', 'Open', 'Close', 'Date']]\n",
    "\n",
    "# Label the target variable based on the opening and closing prices\n",
    "nvidia_df['target'] = np.where(nvidia_df['Open'] > nvidia_df['Close'], 0, 1)\n",
    "\n",
    "# Display the first few rows to verify the merging and labeling\n",
    "print(\"\\nFiltered and Labeled Data (NVIDIA-Related):\")\n",
    "print(nvidia_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf72f648a6d1bf6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T11:58:58.080020700Z",
     "start_time": "2024-10-03T11:58:57.899894600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate documents found: 173\n",
      "Duplicate Documents:\n",
      "                                                 content\n",
      "309    Shares of Uber Technologies Inc  \\r\\n\\r\\n     ...\n",
      "312    Shares of Uber Technologies Inc  \\r\\n\\r\\n     ...\n",
      "625     Bloomberg     You can soon save for your next...\n",
      "640     Bloomberg     You can soon save for your next...\n",
      "5369   Micron  MU  closed the most recent trading day...\n",
      "...                                                  ...\n",
      "68596   Bloomberg     Being an emerging market econom...\n",
      "69812   Bloomberg     A decorated U S Army officer wh...\n",
      "69813   Bloomberg     A decorated U S Army officer wh...\n",
      "70685   Bloomberg     The next U S  jobs report is mo...\n",
      "70686   Bloomberg     The next U S  jobs report is mo...\n",
      "\n",
      "[173 rows x 1 columns]\n",
      "Number of documents after removing duplicates: 71420\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate documents based on the 'content' column\n",
    "duplicate_docs = nvidia_df[nvidia_df['content'].duplicated(keep=False)]\n",
    "\n",
    "# Display the duplicate documents (if any)\n",
    "print(f\"Number of duplicate documents found: {duplicate_docs.shape[0]}\")\n",
    "if not duplicate_docs.empty:\n",
    "    print(\"Duplicate Documents:\")\n",
    "    print(duplicate_docs[['content']])\n",
    "\n",
    "# Remove duplicate documents, keeping the first occurrence\n",
    "nvidia_df = nvidia_df.drop_duplicates(subset='content', keep='first').reset_index(drop=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(f\"Number of documents after removing duplicates: {nvidia_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9de1d67fd4bc2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T12:10:19.889771500Z",
     "start_time": "2024-10-03T11:58:58.086294700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of articles before filtering is: 221513\n",
      "The number of articles after filtering is: 71420\n",
      "The average amount of words per article is: 725.113329599552\n"
     ]
    }
   ],
   "source": [
    "# Basic descriptive statistics for the news dataset\n",
    "num_articles = nvidia_df.shape[0]\n",
    "average_words_per_article = nvidia_df['content'].apply(lambda x: len(str(x).split())).mean()\n",
    "print(f'The number of articles before filtering is: {news_df.shape[0]}')\n",
    "print(f\"The number of articles after filtering is: {num_articles}\")\n",
    "print(f\"The average amount of words per article is: {average_words_per_article}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# Tokenize content for word frequency analysis\n",
    "nvidia_df.loc[:, 'text_length'] = nvidia_df['content'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "\n",
    "# Tokenize content for word frequency analysis\n",
    "nvidia_df.loc[:, 'processed_text'] = nvidia_df['content'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "\n",
    "# Remove stopwords and punctuation for better NLP insights\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Remove stopwords, punctuation, and apply stemming\n",
    "nvidia_df.loc[:, 'filtered_text'] = nvidia_df['processed_text'].apply(\n",
    "    lambda words: [stemmer.stem(word) for word in words if word.isalpha() and word not in stop_words]\n",
    ")\n",
    "\n",
    "\n",
    "# Join the filtered words back into strings for TF-IDF\n",
    "nvidia_df.loc[:, 'filtered_text_str'] = nvidia_df['filtered_text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "544f55b5eff127cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T12:10:50.831530300Z",
     "start_time": "2024-10-03T12:10:19.902955100Z"
    }
   },
   "outputs": [],
   "source": [
    "# TF-IDF Representation of Documents using the processed and filtered text\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500)  # Reduced max features\n",
    "tfidf_matrix = vectorizer.fit_transform(nvidia_df['filtered_text_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cef590aa94dfb51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T12:10:50.843707600Z",
     "start_time": "2024-10-03T12:10:50.841118500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the text data for Word2Vec\n",
    "sentences = nvidia_df['filtered_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eae61a4e64f76943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T12:10:50.880683Z",
     "start_time": "2024-10-03T12:10:50.848789500Z"
    }
   },
   "outputs": [],
   "source": [
    "def document_embedding_tfidf(model, document, tfidf, feature_names, vector_size=100):\n",
    "    \"\"\"\n",
    "    Compute a TF-IDF weighted document embedding by averaging the Word2Vec embeddings of words in the document.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained Word2Vec model\n",
    "    - document: List of tokenized words\n",
    "    - tfidf: TfidfVectorizer object used for the document\n",
    "    - feature_names: The list of features (terms) from the TF-IDF vectorizer\n",
    "    - vector_size: Size of the word embeddings in the Word2Vec model\n",
    "\n",
    "    Returns:\n",
    "    - doc_embedding: The TF-IDF weighted document embedding as a numpy array\n",
    "    \"\"\"\n",
    "    word_vectors = []\n",
    "    weights = []\n",
    "\n",
    "    # Extract the TF-IDF scores for the document\n",
    "    tfidf_scores = tfidf.transform([\" \".join(document)])\n",
    "    tfidf_scores = tfidf_scores.toarray().flatten()\n",
    "\n",
    "    feature_names = feature_names.tolist()  # Convert NumPy array to list\n",
    "\n",
    "    for word in document:\n",
    "        if word in model.wv.index_to_key and word in feature_names:\n",
    "            # Get the word embedding\n",
    "            word_vector = model.wv[word]\n",
    "            word_index = feature_names.index(word)  # Get the index of the word in the TF-IDF feature list\n",
    "\n",
    "            # Get the TF-IDF weight for this word\n",
    "            tfidf_weight = tfidf_scores[word_index]\n",
    "\n",
    "            # Collect the word vector and weight\n",
    "            word_vectors.append(word_vector)\n",
    "            weights.append(tfidf_weight)\n",
    "\n",
    "    if len(word_vectors) == 0:\n",
    "        # Return a zero vector if no words from the document are in the Word2Vec model\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    word_vectors = np.array(word_vectors)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    # Compute the weighted average of word vectors\n",
    "    doc_embedding = np.average(word_vectors, axis=0, weights=weights)\n",
    "\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fbe1bc413181f09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T12:10:50.884576100Z",
     "start_time": "2024-10-03T12:10:50.867117300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nvector_sizes = [100, 200, 300]\\nwindow_sizes = [3, 5, 7]\\nmin_counts = [1, 5, 10]\\nsg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram\\nepochs = [10, 20]\\n'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the grid of parameters to test\n",
    "'''\n",
    "vector_sizes = [100, 200, 300]\n",
    "window_sizes = [3, 5, 7]\n",
    "min_counts = [1, 5, 10]\n",
    "sg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram\n",
    "epochs = [10, 20]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f96c920572f9fd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T20:11:34.108932500Z",
     "start_time": "2024-10-01T20:11:34.014627Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary to store best performance for each model configuration and classifier\n",
    "best_performance = {\n",
    "    'Word2Vec_with_negative_sampling_with_TFIDF': {\n",
    "        'LogisticRegression': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None},\n",
    "        'GaussianNB': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None}\n",
    "    },\n",
    "    'Word2Vec_with_negative_sampling_without_TFIDF': {\n",
    "        'LogisticRegression': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None},\n",
    "        'GaussianNB': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None}\n",
    "    },\n",
    "    'Word2Vec_without_negative_sampling_with_TFIDF': {\n",
    "        'LogisticRegression': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None},\n",
    "        'GaussianNB': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None}\n",
    "    },\n",
    "    'Word2Vec_without_negative_sampling_without_TFIDF': {\n",
    "        'LogisticRegression': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None},\n",
    "        'GaussianNB': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Function to update the best performance for a given configuration and classifier\n",
    "def update_best_performance(model_type, classifier_type, f1, accuracy, params):\n",
    "    if f1 > best_performance[model_type][classifier_type]['best_f1_score']:\n",
    "        best_performance[model_type][classifier_type]['best_f1_score'] = f1\n",
    "        best_performance[model_type][classifier_type]['best_accuracy'] = accuracy\n",
    "        best_performance[model_type][classifier_type]['best_params'] = params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "221c988b31bda76d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T01:48:57.167781100Z",
     "start_time": "2024-10-01T22:37:58.291735300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Word2Vec_with_negative_sampling_with_TFIDF - Logistic Regression, Vector Size: 200, Accuracy: 0.5300, F1 Score: 0.6453\n",
      "Model: Word2Vec_with_negative_sampling_with_TFIDF - Gaussian Naive Bayes, Vector Size: 200, Accuracy: 0.5113, F1 Score: 0.4888\n",
      "Model: Word2Vec_with_negative_sampling_with_TFIDF - Logistic Regression, Vector Size: 200, Accuracy: 0.5307, F1 Score: 0.6445\n",
      "Model: Word2Vec_with_negative_sampling_with_TFIDF - Gaussian Naive Bayes, Vector Size: 200, Accuracy: 0.5106, F1 Score: 0.4872\n",
      "Model: Word2Vec_with_negative_sampling_with_TFIDF - Logistic Regression, Vector Size: 200, Accuracy: 0.5288, F1 Score: 0.6425\n",
      "Model: Word2Vec_with_negative_sampling_with_TFIDF - Gaussian Naive Bayes, Vector Size: 200, Accuracy: 0.5111, F1 Score: 0.4878\n",
      "Model: Word2Vec_with_negative_sampling_with_TFIDF - Logistic Regression, Vector Size: 200, Accuracy: 0.5352, F1 Score: 0.6503\n",
      "Model: Word2Vec_with_negative_sampling_with_TFIDF - Gaussian Naive Bayes, Vector Size: 200, Accuracy: 0.5114, F1 Score: 0.4956\n",
      "Best Performance for Word2Vec_with_negative_sampling_with_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.6503, Accuracy: 0.5352\n",
      "Best Params: (200, 5, 10, 1, 20, 5, True)\n",
      "Best Performance for Word2Vec_with_negative_sampling_with_TFIDF - GaussianNB:\n",
      "F1 Score: 0.4956, Accuracy: 0.5114\n",
      "Best Params: (200, 5, 10, 1, 20, 5, True)\n",
      "Best Performance for Word2Vec_with_negative_sampling_without_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_with_negative_sampling_without_TFIDF - GaussianNB:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_without_negative_sampling_with_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_without_negative_sampling_with_TFIDF - GaussianNB:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_without_negative_sampling_without_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_without_negative_sampling_without_TFIDF - GaussianNB:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "vector_sizes = [200]\n",
    "window_sizes = [3,5]\n",
    "min_counts = [5,10]\n",
    "sg_values = [1]  # 0 for CBOW, 1 for Skip-gram\n",
    "epochs = [20]\n",
    "negative_samples = [5]  # None means no negative sampling\n",
    "use_tfidf = [True]  # For with/without TF-IDF\n",
    "\n",
    "# Initialize TF-IDF vectorizer and compute feature names\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Iterate over all combinations of parameters including TF-IDF and negative sampling\n",
    "for vector_size, window, min_count, sg, epoch, negative, use_tfidf_flag in itertools.product(vector_sizes, window_sizes, min_counts, sg_values, epochs, negative_samples, use_tfidf):\n",
    "    \n",
    "    # Model identifier for storing results\n",
    "    model_type = f'Word2Vec_{\"with\" if negative else \"without\"}_negative_sampling_{\"with_TFIDF\" if use_tfidf_flag else \"without_TFIDF\"}'\n",
    "    \n",
    "    # Initialize Word2Vec with or without negative sampling\n",
    "    if negative is not None:\n",
    "        word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=sg, negative=negative)\n",
    "    else:\n",
    "        word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=sg)\n",
    "    \n",
    "    # Train the model\n",
    "    word2vec_model.train(sentences, total_examples=len(sentences), epochs=epoch)\n",
    "\n",
    "    # Document embeddings, with or without TF-IDF\n",
    "    if use_tfidf_flag:\n",
    "        # Assuming TF-IDF embedding code here\n",
    "        doc_embeddings = nvidia_df['filtered_text'].apply(\n",
    "            lambda doc: document_embedding_tfidf(word2vec_model, doc, vectorizer, tfidf_feature_names, vector_size=vector_size)\n",
    "        ).tolist()\n",
    "    else:\n",
    "        doc_embeddings = [np.mean([word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv], axis=0) for sentence in sentences]\n",
    "\n",
    "    # Check for empty embeddings\n",
    "    X = np.array([embedding for embedding in doc_embeddings if embedding is not None and len(embedding) > 0])\n",
    "    y = nvidia_df['target']\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, stratify=y)\n",
    "\n",
    "    # ==========================\n",
    "    # Train Logistic Regression\n",
    "    # ==========================\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions on the test set\n",
    "    y_pred = lr_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and F1 score for Logistic Regression\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Model: {model_type} - Logistic Regression, Vector Size: {vector_size}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Update best performance for Logistic Regression\n",
    "    update_best_performance(model_type, 'LogisticRegression', f1, accuracy, (vector_size, window, min_count, sg, epoch, negative, use_tfidf_flag))\n",
    "\n",
    "    # ==========================\n",
    "    # Train Gaussian Naive Bayes\n",
    "    # ==========================\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions on the test set\n",
    "    y_pred = nb_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and F1 score for Naive Bayes\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Model: {model_type} - Gaussian Naive Bayes, Vector Size: {vector_size}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Update best performance for Naive Bayes\n",
    "    update_best_performance(model_type, 'GaussianNB', f1, accuracy, (vector_size, window, min_count, sg, epoch, negative, use_tfidf_flag))\n",
    "\n",
    "# After running all configurations, print the best performance for each model type and classifier\n",
    "for model_type, classifiers in best_performance.items():\n",
    "    for classifier_type, performance in classifiers.items():\n",
    "        print(f\"Best Performance for {model_type} - {classifier_type}:\")\n",
    "        print(f\"F1 Score: {performance['best_f1_score']:.4f}, Accuracy: {performance['best_accuracy']:.4f}\")\n",
    "        print(f\"Best Params: {performance['best_params']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da8298598335f879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T01:48:57.167781100Z",
     "start_time": "2024-10-02T01:48:57.161031900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Performance for Word2Vec_with_negative_sampling_with_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.6503, Accuracy: 0.5352\n",
      "Best Params: (200, 5, 10, 1, 20, 5, True)\n",
      "Best Performance for Word2Vec_with_negative_sampling_with_TFIDF - GaussianNB:\n",
      "F1 Score: 0.4956, Accuracy: 0.5114\n",
      "Best Params: (200, 5, 10, 1, 20, 5, True)\n",
      "Best Performance for Word2Vec_with_negative_sampling_without_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_with_negative_sampling_without_TFIDF - GaussianNB:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_without_negative_sampling_with_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_without_negative_sampling_with_TFIDF - GaussianNB:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_without_negative_sampling_without_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_without_negative_sampling_without_TFIDF - GaussianNB:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n"
     ]
    }
   ],
   "source": [
    "# After running all configurations, print the best performance for each model type and classifier\n",
    "for model_type, classifiers in best_performance.items():\n",
    "    for classifier_type, performance in classifiers.items():\n",
    "        print(f\"Best Performance for {model_type} - {classifier_type}:\")\n",
    "        print(f\"F1 Score: {performance['best_f1_score']:.4f}, Accuracy: {performance['best_accuracy']:.4f}\")\n",
    "        print(f\"Best Params: {performance['best_params']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Dictionary to store best performance for each model configuration and classifier\n",
    "best_performance = {\n",
    "    'Word2Vec_with_negative_sampling_with_TFIDF': {\n",
    "        'LogisticRegression': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None},\n",
    "        'GaussianNB': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None}\n",
    "    },\n",
    "    'Word2Vec_with_negative_sampling_without_TFIDF': {\n",
    "        'LogisticRegression': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None},\n",
    "        'GaussianNB': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None}\n",
    "    },\n",
    "    'Word2Vec_without_negative_sampling_with_TFIDF': {\n",
    "        'LogisticRegression': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None},\n",
    "        'GaussianNB': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None}\n",
    "    },\n",
    "    'Word2Vec_without_negative_sampling_without_TFIDF': {\n",
    "        'LogisticRegression': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None},\n",
    "        'GaussianNB': {'best_f1_score': 0, 'best_accuracy': 0, 'best_params': None}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Function to update the best performance for a given configuration and classifier\n",
    "def update_best_performance(model_type, classifier_type, f1, accuracy, params):\n",
    "    if f1 > best_performance[model_type][classifier_type]['best_f1_score']:\n",
    "        best_performance[model_type][classifier_type]['best_f1_score'] = f1\n",
    "        best_performance[model_type][classifier_type]['best_accuracy'] = accuracy\n",
    "        best_performance[model_type][classifier_type]['best_params'] = params\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T12:10:50.974064400Z",
     "start_time": "2024-10-03T12:10:50.880683Z"
    }
   },
   "id": "a6f93e3e74f487d5"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Word2Vec_without_negative_sampling_with_TFIDF - Logistic Regression, Vector Size: 200, Accuracy: 0.5328, F1 Score: 0.6473\n",
      "Model: Word2Vec_without_negative_sampling_with_TFIDF - Gaussian Naive Bayes, Vector Size: 200, Accuracy: 0.5097, F1 Score: 0.4880\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define the grid of parameters to test\n",
    "vector_sizes = [200]\n",
    "window_sizes = [5]\n",
    "min_counts = [10]\n",
    "sg_values = [1]  # 0 for CBOW, 1 for Skip-gram\n",
    "epochs = [20]\n",
    "negative_samples = [None]  # None means no negative sampling\n",
    "use_tfidf = [True]  # For with/without TF-IDF\n",
    "\n",
    "# Initialize TF-IDF vectorizer and compute feature names\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Iterate over all combinations of parameters including TF-IDF and negative sampling\n",
    "for vector_size, window, min_count, sg, epoch, negative, use_tfidf_flag in itertools.product(vector_sizes, window_sizes, min_counts, sg_values, epochs, negative_samples, use_tfidf):\n",
    "    \n",
    "    # Model identifier for storing results\n",
    "    model_type = f'Word2Vec_{\"with\" if negative else \"without\"}_negative_sampling_{\"with_TFIDF\" if use_tfidf_flag else \"without_TFIDF\"}'\n",
    "    \n",
    "    # Initialize Word2Vec with or without negative sampling\n",
    "    if negative is not None:\n",
    "        word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=sg, negative=negative)\n",
    "    else:\n",
    "        word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=sg)\n",
    "    \n",
    "    # Train the model\n",
    "    word2vec_model.train(sentences, total_examples=len(sentences), epochs=epoch)\n",
    "\n",
    "    # Document embeddings, with or without TF-IDF\n",
    "    if use_tfidf_flag:\n",
    "        # Assuming TF-IDF embedding code here\n",
    "        doc_embeddings = nvidia_df['filtered_text'].apply(\n",
    "            lambda doc: document_embedding_tfidf(word2vec_model, doc, vectorizer, tfidf_feature_names, vector_size=vector_size)\n",
    "        ).tolist()\n",
    "    else:\n",
    "        doc_embeddings = [np.mean([word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv], axis=0) for sentence in sentences]\n",
    "\n",
    "    # Check for empty embeddings\n",
    "    X = np.array([embedding for embedding in doc_embeddings if embedding is not None and len(embedding) > 0])\n",
    "    y = nvidia_df['target']\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, stratify=y)\n",
    "\n",
    "    # ==========================\n",
    "    # Train Logistic Regression\n",
    "    # ==========================\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions on the test set\n",
    "    y_pred = lr_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and F1 score for Logistic Regression\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Model: {model_type} - Logistic Regression, Vector Size: {vector_size}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Update best performance for Logistic Regression\n",
    "    update_best_performance(model_type, 'LogisticRegression', f1, accuracy, (vector_size, window, min_count, sg, epoch, negative, use_tfidf_flag))\n",
    "\n",
    "    # ==========================\n",
    "    # Train Gaussian Naive Bayes\n",
    "    # ==========================\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "\n",
    "    # Get predictions on the test set\n",
    "    y_pred = nb_model.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and F1 score for Naive Bayes\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Model: {model_type} - Gaussian Naive Bayes, Vector Size: {vector_size}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Update best performance for Naive Bayes\n",
    "    update_best_performance(model_type, 'GaussianNB', f1, accuracy, (vector_size, window, min_count, sg, epoch, negative, use_tfidf_flag))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T13:06:29.875314100Z",
     "start_time": "2024-10-03T12:10:50.974064400Z"
    }
   },
   "id": "3372c1f6c5171913"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Performance for Word2Vec_with_negative_sampling_with_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_with_negative_sampling_with_TFIDF - GaussianNB:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_with_negative_sampling_without_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_with_negative_sampling_without_TFIDF - GaussianNB:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_without_negative_sampling_with_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.6473, Accuracy: 0.5328\n",
      "Best Params: (200, 5, 10, 1, 20, None, True)\n",
      "Best Performance for Word2Vec_without_negative_sampling_with_TFIDF - GaussianNB:\n",
      "F1 Score: 0.4880, Accuracy: 0.5097\n",
      "Best Params: (200, 5, 10, 1, 20, None, True)\n",
      "Best Performance for Word2Vec_without_negative_sampling_without_TFIDF - LogisticRegression:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n",
      "Best Performance for Word2Vec_without_negative_sampling_without_TFIDF - GaussianNB:\n",
      "F1 Score: 0.0000, Accuracy: 0.0000\n",
      "Best Params: None\n"
     ]
    }
   ],
   "source": [
    "# After running all configurations, print the best performance for each model type and classifier\n",
    "for model_type, classifiers in best_performance.items():\n",
    "    for classifier_type, performance in classifiers.items():\n",
    "        print(f\"Best Performance for {model_type} - {classifier_type}:\")\n",
    "        print(f\"F1 Score: {performance['best_f1_score']:.4f}, Accuracy: {performance['best_accuracy']:.4f}\")\n",
    "        print(f\"Best Params: {performance['best_params']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-03T13:06:29.875314100Z",
     "start_time": "2024-10-03T13:06:29.855396700Z"
    }
   },
   "id": "40e57f930b35beb4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Models below as a first iteration for testing accuracy and F1 score with different parameters and also the code that is functioning correctly. The above model will only be used for the final model. Feel free not to run the models below."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72f95be83233d443"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec74d1e18b126f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-01T20:56:27.712888700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the grid of parameters to test\n",
    "vector_sizes = [200]\n",
    "window_sizes = [3]\n",
    "min_counts = [10]\n",
    "sg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram\n",
    "epochs = [10, 20]\n",
    "\n",
    "# Variables to track the best F1 score and accuracy\n",
    "best_f1_score = 0\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for vector_size, window, min_count, sg, epoch in itertools.product(vector_sizes, window_sizes, min_counts, sg_values, epochs):\n",
    "    # Initialize Word2Vec with current hyperparameters\n",
    "    word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=sg)\n",
    "    \n",
    "    # Train the model\n",
    "    word2vec_model.train(sentences, total_examples=len(sentences), epochs=epoch)\n",
    "    \n",
    "    # Convert sentences to document embeddings\n",
    "    doc_embeddings = [np.mean([word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv], axis=0) for sentence in sentences]\n",
    "    \n",
    "    # Check for empty embeddings\n",
    "    X = np.array([embedding for embedding in doc_embeddings if embedding is not None and len(embedding) > 0])\n",
    "    y = nvidia_df['target']\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, stratify=y)\n",
    "    \n",
    "    # Train a simple classifier (e.g., Logistic Regression) on the embeddings\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred = lr_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Vector Size: {vector_size}, Window: {window}, Min Count: {min_count}, SG: {sg}, Epochs: {epoch}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Track the best model based on F1 score or accuracy\n",
    "    if f1 > best_f1_score:  # Track by F1 score\n",
    "        best_f1_score = f1\n",
    "        best_accuracy = accuracy  # Keep track of the corresponding accuracy as well\n",
    "        best_params = (vector_size, window, min_count, sg, epoch)\n",
    "        best_model = word2vec_model\n",
    "\n",
    "# Print the best model's parameters and scores\n",
    "print(f\"Best Model Parameters: Vector Size: {best_params[0]}, Window: {best_params[1]}, Min Count: {best_params[2]}, SG: {best_params[3]}, Epochs: {best_params[4]}\")\n",
    "print(f\"Logistic Regression: Best Model Accuracy: {best_accuracy:.4f}, Best Model F1 Score: {best_f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548405f93433bac",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-01T20:56:27.721397400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the grid of parameters to test\n",
    "vector_sizes = [200]\n",
    "window_sizes = [3]\n",
    "min_counts = [10]\n",
    "sg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram\n",
    "epochs = [10, 20]\n",
    "\n",
    "# Variables to track the best F1 score and accuracy\n",
    "best_f1_score = 0\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for vector_size, window, min_count, sg, epoch in itertools.product(vector_sizes, window_sizes, min_counts, sg_values, epochs):\n",
    "    # Initialize Word2Vec with current hyperparameters\n",
    "    word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=sg)\n",
    "    \n",
    "    # Train the model\n",
    "    word2vec_model.train(sentences, total_examples=len(sentences), epochs=epoch)\n",
    "    \n",
    "    # Convert sentences to document embeddings by averaging the word vectors\n",
    "    doc_embeddings = [np.mean([word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv], axis=0) for sentence in sentences]\n",
    "    \n",
    "    # Checking for empty embeddings\n",
    "    X = np.array([embedding for embedding in doc_embeddings if embedding is not None and len(embedding) > 0])\n",
    "    y = nvidia_df['target']\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, stratify=y)\n",
    "    \n",
    "    # Initialize and train Naive Bayes on the document embeddings\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the test set\n",
    "    y_pred = nb_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model using accuracy and F1-score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Vector Size: {vector_size}, Window: {window}, Min Count: {min_count}, SG: {sg}, Epochs: {epoch}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Track the best model based on F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_accuracy = accuracy  # Track the corresponding accuracy for the best F1 score\n",
    "        best_params = (vector_size, window, min_count, sg, epoch)\n",
    "        best_model = word2vec_model\n",
    "\n",
    "# Print the best model's parameters and scores\n",
    "print(f\"Best Model Parameters: Vector Size: {best_params[0]}, Window: {best_params[1]}, Min Count: {best_params[2]}, SG: {best_params[3]}, Epochs: {best_params[4]}\")\n",
    "print(f\"Gaussian Naive Bayes: Best Model Accuracy: {best_accuracy:.4f}, Best Model F1 Score: {best_f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5385687b6bec91",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-01T20:56:27.721397400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the grid of parameters to test\n",
    "vector_sizes = [200]\n",
    "window_sizes = [3]\n",
    "min_counts = [10]\n",
    "sg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram\n",
    "epochs = [10, 20]\n",
    "\n",
    "# Initialize TF-IDF vectorizer and compute feature names\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "best_score = 0\n",
    "best_f1_score = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "for vector_size, window, min_count, sg, epoch in itertools.product(vector_sizes, window_sizes, min_counts, sg_values, epochs):\n",
    "    # Initialize Word2Vec with current hyperparameters\n",
    "    word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=sg)\n",
    "    \n",
    "    # Train the model\n",
    "    word2vec_model.train(sentences, total_examples=len(sentences), epochs=epoch)\n",
    "\n",
    "    # Generate document embeddings using TF-IDF weighted Word2Vec\n",
    "    doc_embeddings = nvidia_df['filtered_text'].apply(\n",
    "        lambda doc: document_embedding_tfidf(word2vec_model, doc, vectorizer, tfidf_feature_names, vector_size=vector_size)\n",
    "    ).tolist()\n",
    "    \n",
    "    # Ensure embeddings have no None or empty values\n",
    "    X = np.array([embedding for embedding in doc_embeddings if embedding is not None and len(embedding) > 0])\n",
    "    y = nvidia_df['target']\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, stratify=y)\n",
    "    \n",
    "    # Train a simple classifier (e.g., Logistic Regression) on the embeddings\n",
    "    lr_model = LogisticRegression(max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predictions on the test set\n",
    "    y_pred = lr_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Vector Size: {vector_size}, Window: {window}, Min Count: {min_count}, SG: {sg}, Epochs: {epoch}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Track the best model based on F1 score\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_score = accuracy\n",
    "        best_params = (vector_size, window, min_count, sg, epoch)\n",
    "        best_model = word2vec_model\n",
    "\n",
    "print(f\"Best Model Parameters: Vector Size: {best_params[0]}, Window: {best_params[1]}, Min Count: {best_params[2]}, SG: {best_params[3]}, Epochs: {best_params[4]}\")\n",
    "print(f\"Logistic Regression: Best Model Accuracy: {best_score:.4f}, Best Model F1 score: {best_f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f675327bdfc4648",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-01T20:56:27.721397400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the grid of parameters to test\n",
    "vector_sizes = [200]\n",
    "window_sizes = [3]\n",
    "min_counts = [10]\n",
    "sg_values = [0, 1]  # 0 for CBOW, 1 for Skip-gram\n",
    "epochs = [10, 20]\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "best_f1_score = 0\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "# Initialize TF-IDF vectorizer and compute feature names\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for vector_size, window, min_count, sg, epoch in itertools.product(vector_sizes, window_sizes, min_counts, sg_values, epochs):\n",
    "    # Initialize Word2Vec with current hyperparameters\n",
    "    word2vec_model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4, sg=sg)\n",
    "    \n",
    "    # Train the model\n",
    "    word2vec_model.train(sentences, total_examples=len(sentences), epochs=epoch)\n",
    "    \n",
    "    # Generate document embeddings using TF-IDF weighted Word2Vec\n",
    "    doc_embeddings = nvidia_df['filtered_text'].apply(\n",
    "        lambda doc: document_embedding_tfidf(word2vec_model, doc, vectorizer, tfidf_feature_names, vector_size=vector_size)\n",
    "    ).tolist()\n",
    "    \n",
    "    # Checking empty embeddings\n",
    "    X = np.array([embedding for embedding in doc_embeddings if embedding is not None and len(embedding) > 0])\n",
    "    y = nvidia_df['target']\n",
    "    \n",
    "    # Splitting the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, stratify=y)\n",
    "    \n",
    "    # Initializing and training Naive Bayes on the document embeddings\n",
    "    nb_model = GaussianNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the test set\n",
    "    y_pred = nb_model.predict(X_test)\n",
    "    \n",
    "    # Evaluating the model using accuracy and F1-score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Vector Size: {vector_size}, Window: {window}, Min Count: {min_count}, SG: {sg}, Epochs: {epoch}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Tracking the best model based on F1-score (or accuracy)\n",
    "    if f1 > best_f1_score:  # Track by F1 score\n",
    "        best_f1_score = f1\n",
    "        best_accuracy = accuracy  # Keep track of the corresponding accuracy as well\n",
    "        best_params = (vector_size, window, min_count, sg, epoch)\n",
    "        best_model = word2vec_model\n",
    "\n",
    "print(f\"Best Model Parameters: Vector Size: {best_params[0]}, Window: {best_params[1]}, Min Count: {best_params[2]}, SG: {best_params[3]}, Epochs: {best_params[4]}\")\n",
    "print(f\"Gaussian Naive Bayes: Best Model Accuracy: {best_accuracy:.4f}, Best Model F1 Score: {best_f1_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
