{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Source: https://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick\n",
    "# Source: https://stackoverflow.com/questions/17536394/how-can-i-reduce-memory-usage-of-scikit-learn-vectorizers\n",
    "# Reasoning is that TFIDF Vectorizer create very sparse matrix, which resulted in a 284GB matrix that needed to be held in memory in order to process it.\n",
    "# Initialize HashingVectorizer with a reasonable number of features\n",
    "hash_vectorizer = HashingVectorizer(n_features=2**10, alternate_sign=False, norm=None)\n",
    "\n",
    "# Transform the text data into hashed feature space using the string format\n",
    "hashed_matrix = hash_vectorizer.transform(filtered_df['filtered_text_str'])\n",
    "\n",
    "# Apply TfidfTransformer to add IDF weighting\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(hashed_matrix)\n",
    "\n",
    "# Check the shape and sparsity of the transformed TF-IDF matrix\n",
    "print(f\"Shape of tfidf_matrix: {tfidf_matrix.shape}\")\n",
    "print(f\"Number of non-zero elements: {tfidf_matrix.nnz}\")\n",
    "print(f\"Sparsity: {(1 - (tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]))) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute cosine similarity matrix, which measures the similarity between documents\n",
    "# Source: https://medium.com/@anurag-jain/tf-idf-vectorization-with-cosine-similarity-eca3386d4423\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d62c9d0535d437ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use TfidfVectorizer with min_df and ngram_range to reduce dimensionality\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase = False,  # Do not convert to lowercase,because it is already converted\n",
    "    min_df = 2,  # Ignore terms that appear in less than 2 documents\n",
    "    ngram_range=(1, 2),  # Consider both unigrams and bigrams\n",
    "    max_features=10000,  # Further limit the size of the vocabulary\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "#tfidf_matrix = vectorizer.fit_transform(filtered_df['filtered_text_str'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5696ded29eaf1ac5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    max_features=10,\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(filtered_df['filtered_text_str'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70413a9c70bdaef1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Train a simple logistic regression model for demonstration\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(tfidf_matrix, filtered_df['target'])\n",
    "\n",
    "# Function to print top informative features for binary classification\n",
    "def print_top_features(vectorizer, clf, top_n=20):\n",
    "    \"\"\"Prints the top n informative features for each class using model coefficients.\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coef = clf.coef_[0]  # Use the first (and only) set of coefficients for binary classification\n",
    "    \n",
    "    # Top features with the most negative coefficients (indicative of Class 0)\n",
    "    top_features_class_0 = np.argsort(coef)[:top_n]\n",
    "    print(\"\\nTop 20 Words Indicative of Class 0 (Stock Price Down):\")\n",
    "    print(\" \".join(feature_names[j] for j in top_features_class_0))\n",
    "    \n",
    "    # Top features with the most positive coefficients (indicative of Class 1)\n",
    "    top_features_class_1 = np.argsort(coef)[-top_n:]\n",
    "    print(\"\\nTop 20 Words Indicative of Class 1 (Stock Price Up):\")\n",
    "    print(\" \".join(feature_names[j] for j in top_features_class_1))\n",
    "    \n",
    "    return top_features_class_0, top_features_class_1\n",
    "\n",
    "# Print the most informative features\n",
    "top_features_class_0, top_features_class_1 = print_top_features(vectorizer, clf)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e23ff523a13c355"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the most indicative words for Class 0\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh([vectorizer.get_feature_names_out()[i] for i in top_features_class_0], clf.coef_[0][top_features_class_0], color='blue')\n",
    "plt.title('Top 20 Words Indicative of Class 0 (Stock Price Down)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "231c452ff5ad93c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the most indicative words for Class 1\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh([vectorizer.get_feature_names_out()[i] for i in top_features_class_1], clf.coef_[0][top_features_class_1], color='green')\n",
    "plt.title('Top 20 Words Indicative of Class 1 (Stock Price Up)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c40924edd95537"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the shape of the tfidf_matrix\n",
    "print(f\"Shape of tfidf_matrix: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Number of documents (rows) and terms (columns)\n",
    "num_documents, num_features = tfidf_matrix.shape\n",
    "print(f\"Number of documents: {num_documents}\")\n",
    "print(f\"Number of features (terms): {num_features}\")\n",
    "\n",
    "# Check the number of non-zero elements\n",
    "non_zero_elements = tfidf_matrix.nnz\n",
    "print(f\"Number of non-zero elements: {non_zero_elements}\")\n",
    "\n",
    "# Calculate the total number of elements\n",
    "total_elements = num_documents * num_features\n",
    "print(f\"Total number of elements: {total_elements}\")\n",
    "\n",
    "# Calculate the sparsity of the matrix\n",
    "sparsity = (1 - (non_zero_elements / total_elements)) * 100\n",
    "print(f\"Sparsity of the tfidf_matrix: {sparsity:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab3c986d53b636a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute cosine similarity matrix, which measures the similarity between documents\n",
    "# Source: https://medium.com/@anurag-jain/tf-idf-vectorization-with-cosine-similarity-eca3386d4423\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Convert similarity matrix to a DataFrame for easier handling\n",
    "similarity_df = pd.DataFrame(similarity_matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d540b268ec0d05c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity in batches\n",
    "def compute_similarity_in_batches(tfidf_matrix, batch_size=1000):\n",
    "    num_docs = tfidf_matrix.shape[0]\n",
    "    similarity_results = []\n",
    "    \n",
    "    # Process the similarity computation in batches\n",
    "    for start in range(0, num_docs, batch_size):\n",
    "        end = min(start + batch_size, num_docs)\n",
    "        print(f\"Processing batch: {start} to {end}\")\n",
    "        \n",
    "        # Compute similarities for the batch\n",
    "        batch_similarity = cosine_similarity(tfidf_matrix[start:end], tfidf_matrix)\n",
    "        \n",
    "        # Convert to a sparse matrix to save memory (optional)\n",
    "        batch_similarity_sparse = pd.DataFrame(batch_similarity)\n",
    "        \n",
    "        # Append the results\n",
    "        similarity_results.append(batch_similarity_sparse)\n",
    "    \n",
    "    # Combine all batches into one DataFrame\n",
    "    return pd.concat(similarity_results, axis=0)\n",
    "\n",
    "# Using the function to compute the similarity matrix in smaller chunks\n",
    "similarity_df = compute_similarity_in_batches(tfidf_matrix, batch_size=500)\n",
    "\n",
    "# Display the shape to confirm the complete matrix\n",
    "print(f\"Computed similarity matrix shape: {similarity_df.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3354230d9fabbf42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find indices of the most and least similar documents (excluding self-similarity)\n",
    "np.fill_diagonal(similarity_matrix, 0)\n",
    "most_similar_indices = np.unravel_index(np.argmax(similarity_matrix, axis=None), similarity_matrix.shape)\n",
    "least_similar_indices = np.unravel_index(np.argmin(similarity_matrix, axis=None), similarity_matrix.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a34cb3f4f9e27fe9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Inspect the most similar documents\n",
    "similar_doc_1 = news_df.iloc[most_similar_indices[0]]\n",
    "similar_doc_2 = news_df.iloc[most_similar_indices[1]]\n",
    "\n",
    "print(\"\\nMost Similar Documents:\\n\")\n",
    "print(\"Document 1:\")\n",
    "print(similar_doc_1['content'])\n",
    "print(\"\\nDocument 2:\")\n",
    "print(similar_doc_2['content'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f29ec6f81e3eb80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Inspect the least similar documents\n",
    "dissimilar_doc_1 = news_df.iloc[least_similar_indices[0]]\n",
    "dissimilar_doc_2 = news_df.iloc[least_similar_indices[1]]\n",
    "\n",
    "print(\"\\n\\nMost Dissimilar Documents:\\n\")\n",
    "print(\"Document 1:\")\n",
    "print(dissimilar_doc_1['content'])\n",
    "print(\"\\nDocument 2:\")\n",
    "print(dissimilar_doc_2['content'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e55b68340a07020"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2Vec Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d930254748ae1aa6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare the text data for Word2Vec\n",
    "sentences = filtered_df['filtered_text'].tolist()\n",
    "# Initializing Word2Vec model \n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=0)  # sg=0 for CBOW"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eed8f14fd29dfcf5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training the model\n",
    "word2vec_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "# Saving the model for future use\n",
    "word2vec_model.save(\"word2vec_model.bin\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3904768e2feea71a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Getting word vectors for a particular word\n",
    "word_vector = word2vec_model.wv['nvidia']\n",
    "\n",
    "# Converting a document to an embedding by averaging word vectors\n",
    "def get_document_embedding(doc):\n",
    "    return np.mean([word2vec_model.wv[word] for word in doc if word in word2vec_model.wv], axis=0)\n",
    "\n",
    "# Applying to the dataset\n",
    "filtered_df['doc_embedding'] = filtered_df['filtered_text'].apply(get_document_embedding)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53313e3aea0138d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a simple logistic regression model for demonstration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15ecbc922e4bbe9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Feature Matrix and Target Variable\n",
    "X = np.vstack(filtered_df['doc_embedding'].values)\n",
    "y = filtered_df['target']\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, stratify=y)\n",
    "\n",
    "# Optional: Apply scaling to improve model performance (though it's not always necessary for TF-IDF)\n",
    "# Initialize the scaler and create a pipeline with scaling and the classifier\n",
    "scaler = StandardScaler(with_mean=False)  # with_mean=False because TF-IDF produces sparse matrices\n",
    "\n",
    "# Example with Logistic Regression\n",
    "lr_pipeline = make_pipeline(scaler, LogisticRegression(max_iter=1000))\n",
    "\n",
    "# Train the model\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_score = lr_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy of Logistic Regression: {test_score:.2f}\")\n",
    "\n",
    "# Optionally, use cross-validation to validate the model across multiple folds\n",
    "cross_val_scores = cross_val_score(lr_pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validated accuracy: {np.mean(cross_val_scores):.2f} ± {np.std(cross_val_scores):.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9ca4940c61edfab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a simple logistic regression model for demonstration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32daf37c99ee975b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB ( Multinomial can't handle negative values)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Use the document embeddings (X_train should be the embedding matrix)\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb9ab85202b0acd2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50bdad4d1baf3cfe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predictions for both models\n",
    "gnb_pred = gnb_model.predict(X_test)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy_nb = accuracy_score(y_test, gnb_pred)\n",
    "accuracy_lr = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "f1_nb = f1_score(y_test, gnb_pred)\n",
    "f1_lr = f1_score(y_test, lr_pred)\n",
    "\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb:.2f}, F1 Score: {f1_nb:.2f}\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr:.2f}, F1 Score: {f1_lr:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_nb = confusion_matrix(y_test, gnb_pred)\n",
    "cm_lr = confusion_matrix(y_test, lr_pred)\n",
    "\n",
    "print(\"\\nNaive Bayes Confusion Matrix:\")\n",
    "print(cm_nb)\n",
    "\n",
    "print(\"\\nLogistic Regression Confusion Matrix:\")\n",
    "print(cm_lr)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69f665cac610f6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initializing Word2Vec model \n",
    "word2vec_model = Word2Vec(sentences, vector_size=200, window=5, min_count=1, workers=4, sg=1)  # sg=1 for Skip-Gram"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4efa08c2be00aaec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training the model\n",
    "word2vec_model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "# Saving the model for future use\n",
    "word2vec_model.save(\"word2vec_model_skipgram.bin\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fad8630c1f9c86c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting word vectors for a particular word\n",
    "word_vector_2 = word2vec_model.wv['nvidia']\n",
    "\n",
    "# Converting a document to an embedding by averaging word vectors\n",
    "def get_document_embedding(doc):\n",
    "    return np.mean([word2vec_model.wv[word] for word in doc if word in word2vec_model.wv], axis=0)\n",
    "\n",
    "# Applying to the dataset\n",
    "filtered_df['doc_embedding'] = filtered_df['filtered_text'].apply(get_document_embedding)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f5ed64879101261"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply the function to all documents in filtered dataframe\n",
    "filtered_df['doc_embedding_skip'] = filtered_df['filtered_text'].apply(\n",
    "    lambda doc: document_embedding_tfidf(word2vec_model, doc, vectorizer, tfidf_feature_names)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e3287cb6a30a444"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Prepare your feature matrix and target\n",
    "X = filtered_df['doc_embedding'].tolist()  # Assuming you're using document embeddings\n",
    "X = np.array(X)  # Convert list of embeddings to numpy array\n",
    "y = filtered_df['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17, stratify=y)\n",
    "\n",
    "# Define the parameter grid for var_smoothing\n",
    "param_grid = {'var_smoothing': np.logspace(0, -9, num=100)}\n",
    "\n",
    "# Initialize the GaussianNB model\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(gnb_model, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_score:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_gnb_model = grid_search.best_estimator_\n",
    "y_pred = best_gnb_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b2f740c8dcef11d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB ( Multinomial can't handle negative values)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Use the document embeddings (X_train should be the embedding matrix)\n",
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(X_train, y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "548e7bb74d33261c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predictions for both models\n",
    "gnb_pred = gnb_model.predict(X_test)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy_nb = accuracy_score(y_test, gnb_pred)\n",
    "accuracy_lr = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "f1_nb = f1_score(y_test, gnb_pred)\n",
    "f1_lr = f1_score(y_test, lr_pred)\n",
    "\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb:.2f}, F1 Score: {f1_nb:.2f}\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr:.2f}, F1 Score: {f1_lr:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_nb = confusion_matrix(y_test, gnb_pred)\n",
    "cm_lr = confusion_matrix(y_test, lr_pred)\n",
    "\n",
    "print(\"\\nNaive Bayes Confusion Matrix:\")\n",
    "print(cm_nb)\n",
    "\n",
    "print(\"\\nLogistic Regression Confusion Matrix:\")\n",
    "print(cm_lr)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62448037a028fa3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Step 2: Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90a48cdd2818d7e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
