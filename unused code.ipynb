{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Source: https://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick\n",
    "# Source: https://stackoverflow.com/questions/17536394/how-can-i-reduce-memory-usage-of-scikit-learn-vectorizers\n",
    "# Reasoning is that TFIDF Vectorizer create very sparse matrix, which resulted in a 284GB matrix that needed to be held in memory in order to process it.\n",
    "# Initialize HashingVectorizer with a reasonable number of features\n",
    "hash_vectorizer = HashingVectorizer(n_features=2**10, alternate_sign=False, norm=None)\n",
    "\n",
    "# Transform the text data into hashed feature space using the string format\n",
    "hashed_matrix = hash_vectorizer.transform(filtered_df['filtered_text_str'])\n",
    "\n",
    "# Apply TfidfTransformer to add IDF weighting\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(hashed_matrix)\n",
    "\n",
    "# Check the shape and sparsity of the transformed TF-IDF matrix\n",
    "print(f\"Shape of tfidf_matrix: {tfidf_matrix.shape}\")\n",
    "print(f\"Number of non-zero elements: {tfidf_matrix.nnz}\")\n",
    "print(f\"Sparsity: {(1 - (tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]))) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute cosine similarity matrix, which measures the similarity between documents\n",
    "# Source: https://medium.com/@anurag-jain/tf-idf-vectorization-with-cosine-similarity-eca3386d4423\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d62c9d0535d437ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use TfidfVectorizer with min_df and ngram_range to reduce dimensionality\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase = False,  # Do not convert to lowercase,because it is already converted\n",
    "    min_df = 2,  # Ignore terms that appear in less than 2 documents\n",
    "    ngram_range=(1, 2),  # Consider both unigrams and bigrams\n",
    "    max_features=10000,  # Further limit the size of the vocabulary\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "#tfidf_matrix = vectorizer.fit_transform(filtered_df['filtered_text_str'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5696ded29eaf1ac5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    max_features=10,\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(filtered_df['filtered_text_str'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70413a9c70bdaef1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Train a simple logistic regression model for demonstration\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(tfidf_matrix, filtered_df['target'])\n",
    "\n",
    "# Function to print top informative features for binary classification\n",
    "def print_top_features(vectorizer, clf, top_n=20):\n",
    "    \"\"\"Prints the top n informative features for each class using model coefficients.\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coef = clf.coef_[0]  # Use the first (and only) set of coefficients for binary classification\n",
    "    \n",
    "    # Top features with the most negative coefficients (indicative of Class 0)\n",
    "    top_features_class_0 = np.argsort(coef)[:top_n]\n",
    "    print(\"\\nTop 20 Words Indicative of Class 0 (Stock Price Down):\")\n",
    "    print(\" \".join(feature_names[j] for j in top_features_class_0))\n",
    "    \n",
    "    # Top features with the most positive coefficients (indicative of Class 1)\n",
    "    top_features_class_1 = np.argsort(coef)[-top_n:]\n",
    "    print(\"\\nTop 20 Words Indicative of Class 1 (Stock Price Up):\")\n",
    "    print(\" \".join(feature_names[j] for j in top_features_class_1))\n",
    "    \n",
    "    return top_features_class_0, top_features_class_1\n",
    "\n",
    "# Print the most informative features\n",
    "top_features_class_0, top_features_class_1 = print_top_features(vectorizer, clf)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e23ff523a13c355"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the most indicative words for Class 0\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh([vectorizer.get_feature_names_out()[i] for i in top_features_class_0], clf.coef_[0][top_features_class_0], color='blue')\n",
    "plt.title('Top 20 Words Indicative of Class 0 (Stock Price Down)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "231c452ff5ad93c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the most indicative words for Class 1\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh([vectorizer.get_feature_names_out()[i] for i in top_features_class_1], clf.coef_[0][top_features_class_1], color='green')\n",
    "plt.title('Top 20 Words Indicative of Class 1 (Stock Price Up)')\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c40924edd95537"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the shape of the tfidf_matrix\n",
    "print(f\"Shape of tfidf_matrix: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Number of documents (rows) and terms (columns)\n",
    "num_documents, num_features = tfidf_matrix.shape\n",
    "print(f\"Number of documents: {num_documents}\")\n",
    "print(f\"Number of features (terms): {num_features}\")\n",
    "\n",
    "# Check the number of non-zero elements\n",
    "non_zero_elements = tfidf_matrix.nnz\n",
    "print(f\"Number of non-zero elements: {non_zero_elements}\")\n",
    "\n",
    "# Calculate the total number of elements\n",
    "total_elements = num_documents * num_features\n",
    "print(f\"Total number of elements: {total_elements}\")\n",
    "\n",
    "# Calculate the sparsity of the matrix\n",
    "sparsity = (1 - (non_zero_elements / total_elements)) * 100\n",
    "print(f\"Sparsity of the tfidf_matrix: {sparsity:.2f}%\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab3c986d53b636a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute cosine similarity matrix, which measures the similarity between documents\n",
    "# Source: https://medium.com/@anurag-jain/tf-idf-vectorization-with-cosine-similarity-eca3386d4423\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Convert similarity matrix to a DataFrame for easier handling\n",
    "similarity_df = pd.DataFrame(similarity_matrix)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d540b268ec0d05c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity in batches\n",
    "def compute_similarity_in_batches(tfidf_matrix, batch_size=1000):\n",
    "    num_docs = tfidf_matrix.shape[0]\n",
    "    similarity_results = []\n",
    "    \n",
    "    # Process the similarity computation in batches\n",
    "    for start in range(0, num_docs, batch_size):\n",
    "        end = min(start + batch_size, num_docs)\n",
    "        print(f\"Processing batch: {start} to {end}\")\n",
    "        \n",
    "        # Compute similarities for the batch\n",
    "        batch_similarity = cosine_similarity(tfidf_matrix[start:end], tfidf_matrix)\n",
    "        \n",
    "        # Convert to a sparse matrix to save memory (optional)\n",
    "        batch_similarity_sparse = pd.DataFrame(batch_similarity)\n",
    "        \n",
    "        # Append the results\n",
    "        similarity_results.append(batch_similarity_sparse)\n",
    "    \n",
    "    # Combine all batches into one DataFrame\n",
    "    return pd.concat(similarity_results, axis=0)\n",
    "\n",
    "# Using the function to compute the similarity matrix in smaller chunks\n",
    "similarity_df = compute_similarity_in_batches(tfidf_matrix, batch_size=500)\n",
    "\n",
    "# Display the shape to confirm the complete matrix\n",
    "print(f\"Computed similarity matrix shape: {similarity_df.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3354230d9fabbf42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find indices of the most and least similar documents (excluding self-similarity)\n",
    "np.fill_diagonal(similarity_matrix, 0)\n",
    "most_similar_indices = np.unravel_index(np.argmax(similarity_matrix, axis=None), similarity_matrix.shape)\n",
    "least_similar_indices = np.unravel_index(np.argmin(similarity_matrix, axis=None), similarity_matrix.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a34cb3f4f9e27fe9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Inspect the most similar documents\n",
    "similar_doc_1 = news_df.iloc[most_similar_indices[0]]\n",
    "similar_doc_2 = news_df.iloc[most_similar_indices[1]]\n",
    "\n",
    "print(\"\\nMost Similar Documents:\\n\")\n",
    "print(\"Document 1:\")\n",
    "print(similar_doc_1['content'])\n",
    "print(\"\\nDocument 2:\")\n",
    "print(similar_doc_2['content'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f29ec6f81e3eb80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Inspect the least similar documents\n",
    "dissimilar_doc_1 = news_df.iloc[least_similar_indices[0]]\n",
    "dissimilar_doc_2 = news_df.iloc[least_similar_indices[1]]\n",
    "\n",
    "print(\"\\n\\nMost Dissimilar Documents:\\n\")\n",
    "print(\"Document 1:\")\n",
    "print(dissimilar_doc_1['content'])\n",
    "print(\"\\nDocument 2:\")\n",
    "print(dissimilar_doc_2['content'])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e55b68340a07020"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
